import pandas as pd
    from datetime import timedelta
    from io import StringIO

    def process_execution_function(data, id_col, date_col, order_id_col, value_col):
        data[date_col] = pd.to_datetime(data[date_col])
        last_run_date = data[date_col].max() + timedelta(days=1)
        # Grouping by CustomerID
        data_process = data.groupby([id_col]).agg({
                date_col: lambda x: (last_run_date - x.max()).days,
                order_id_col: 'count',
                value_col: 'sum'})
        # Rename the columns 
        data_process.rename(columns={date_col: 'Recency',
                                 order_id_col: 'Frequency',
                                 value_col: 'MonetaryValue'}, inplace=True)


        r_labels = range(4, 0, -1); f_labels = range(1, 5)
        # Assign these labels to 4 equal percentile groups 
        r_groups = pd.qcut(data_process['Recency'].rank(method='first'), q=4, labels=r_labels, duplicates='drop')
        # Assign these labels to 4 equal percentile groups 
        f_groups = pd.qcut(data_process['Frequency'].rank(method='first'), q=4, labels=f_labels, duplicates='drop')
        # Create new columns R and F 
        data_process = data_process.assign(R = r_groups.values, F = f_groups.values)
        data_process.head()
        # Create labels for MonetaryValue
        m_labels = range(1, 5)
        # Assign these labels to four equal percentile groups 
        m_groups = pd.qcut(data_process['MonetaryValue'].rank(method='first'), q=4, labels=m_labels, duplicates='drop')
        # Create new column M
        data_process = data_process.assign(M = m_groups.values)
        return data_process


    def join_rfm(x): 
        return str(x['R']) + str(x['F']) + str(x['M'])

    #Specify the Path
    data=pd.read_csv(DatasetPath)
    data['OrderReceivedDate'] = pd.to_datetime(data['OrderReceivedDate'])
    #Change corresponding columns as required for RFM Analysis
    id_col='ShipToName'
    date_col='OrderReceivedDate'
    order_id_col='OrderNumber'
    value_col='Adj_OrderNetValue'

    data_process=process_execution_function(data,id_col,date_col, order_id_col, value_col)

    data_process['R']=data_process.loc[:,"R"].apply(lambda x : 5-x)
    data_process['F']=data_process.loc[:,"F"].apply(lambda x : 5-x)
    data_process['M']=data_process.loc[:,"M"].apply(lambda x : 5-x)
    data_process['RFM'] = data_process.apply(join_rfm, axis=1)

    data_process['RFM'] = data_process['RFM'].astype(str)
    data_process['R'] = data_process['R'].astype(str)
    data_process['F'] = data_process['F'].astype(str)
    data_process['M'] = data_process['M'].astype(str)

    data_process.loc[:,"CustomerName"]=data_process.index
    data_process=data_process.reset_index(drop=True)

    data_process.loc[:,"Segment"]='Others'
    indices=data_process[data_process.loc[:,"R"]=='1'].index
    data_process.loc[indices, "Segment"]="Recent Customers"

    indices=data_process[data_process.loc[:,"F"]=='1'].index
    data_process.loc[indices, "Segment"]="Loyal Customers"

    indices=data_process[data_process.loc[:,"M"]=='1'].index
    data_process.loc[indices,"Segment"]="Big Spenders"

    indices=data_process[data_process.loc[:,"RFM"]=='111'].index
    data_process.loc[indices,"Segment"]="Best Customers"

    indices=data_process[data_process.loc[:,"RFM"]=='311'].index
    data_process.loc[indices,"Segment"]="Almost Lost Customers"

    indices=data_process[data_process.loc[:,"RFM"]=='411'].index
    data_process.loc[indices,"Segment"]="Lost Customers"

    indices=data_process[data_process.loc[:,"RFM"]=='444'].index
    data_process.loc[indices,"Segment"]="Lost Cheap Customers"
    #data_process.to_csv("s3://sagemaker-us-east-1-754307369999/Output/RFM Data.csv", index=None)
    StringData = StringIO() 
    data_process.to_csv(StringData)
    s3_resource = boto3.resource('s3')
    s3_resource.Object(bucket_name='sagemaker-us-east-1-754307369999',key='Output/RfmOutput.csv').put(Body=StringData.getvalue())
