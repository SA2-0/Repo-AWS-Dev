{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "import time\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from io import StringIO\n",
    "import time\n",
    "\n",
    "sess = boto3.Session()\n",
    "sm = sess.client('sagemaker')\n",
    "import psycopg2 as pg\n",
    "import pandas.io.sql as psql\n",
    "import pandas as pd\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('rds')\n",
    "response = client.describe_db_instances(\n",
    "    DBInstanceIdentifier='sa2-db'\n",
    ")\n",
    "print(response['DBInstances'][0]['Endpoint']['Address'])\n",
    "hostname=response['DBInstances'][0]['Endpoint']['Address']\n",
    "connection = pg.connect(user = 'postgres',\n",
    "password = 'sa2dbroot',\n",
    "host = str(hostname),\n",
    "port = '5432',\n",
    "database = 'yash_sa_schema')\n",
    "#dataframe = psql.DataFrame(\"SELECT * FROM category\", connection)\n",
    "data = pd.read_sql_query('SELECT * FROM yash_sa_schema.sa_file_ingst_log ORDER BY sa_file_ingst_log.upld_ts DESC LIMIT 1',con=connection)\n",
    "print(data['published_file_path'].iloc[0])\n",
    "#print(data['data_src_nm'].iloc[0])\n",
    "print(data['published_file_name'].iloc[0])\n",
    "print(data['model'].iloc[0])\n",
    "Type=data['file_upld_typ'].iloc[0]\n",
    "model=data['model'].iloc[0]\n",
    "DatasetPath=data['published_file_path'].iloc[0]\n",
    "filename=data['published_file_name'].iloc[0]\n",
    "#DatasetPath=data['DatasetPath'].iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "new_role = sagemaker.get_execution_role()\n",
    "#change the model_data parameter below to the location of tar.gz containing joblib/pickle\n",
    "model = SKLearnModel(model_data=DatasetPath,entry_point='entry_point.py',source_dir='./DataFolder',role = new_role)\n",
    "\n",
    "#model = SKLearnModel(model_data=\"s3://sagemaker-us-east-1-754307369999/test_joblib_deployment/model.tar.gz\",entry_point='model_entry_point_v1.py',source_dir='./DataFolder',role = new_role)\n",
    "# entry point script in ./DataFolder has version v1 for joblib and v2 for pickle seperately. ./DatFolder also contains requirements.txt for dependencies.\n",
    "\n",
    "#model = SKLearnModel(model_data=\"s3://sagemaker-us-east-1-754307369999/test_joblib_deployment/model3.tar.gz\",entry_point='model_entry_point_v2.py',source_dir='./DataFolder',role = new_role)\n",
    "# similar model objects need to be created depending upon the framework used to train the model. the entry_point script loads the model from joblib/pickle and returns the model object.\n",
    "\n",
    "predictor = model.deploy(instance_type=\"ml.m5.large\", initial_instance_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
